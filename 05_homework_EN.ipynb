{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalue23/Exercises-Uni/blob/main/05_homework_EN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework for Session 5"
      ],
      "metadata": {
        "id": "PfF0ezEM6k5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tasks presented here relate to the contents of the notebook [Unsupervised Learning](https://colab.research.google.com/drive/1wWG76ET42fxPAMAQW1Cjk2OTIcSwl7Mw?usp=sharing).\n",
        "\n",
        "Remember: **To keep any changes you make to the notebook, you must first save a copy for yourself. To do this, go to `File` â†’ `Save a copy in Drive`.** If you are using a local Python installation, download the notebook before making any changes."
      ],
      "metadata": {
        "id": "mnzv0n867FCv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1\n",
        "In this exercise, we want to explore the farm data a bit more closely using Principal Component Analysis (PCA).\n",
        "* How much variance is explained by the first component, and how much by the second component? Consequently, how much variance is **not** explained by the first two principal components? What can you conclude from this about how successfully PCA was able to shift the variance into the first two principal components?\n",
        "* Examine which features are particularly important for the first two principal components. Filter for features whose importance is >0.1 or < -0.1, as otherwise there will be too many features. Display the importance of the features for the first and second principal components (PC1 and PC2) each in a bar chart."
      ],
      "metadata": {
        "id": "fCAZzfJZE9Vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we load the farm dataset ...\n",
        "import pandas as pd\n",
        "url = \"https://drive.google.com/uc?id=1mNO7yf89ReYPvjJgfD3YdY5MdIVGvCtp\"\n",
        "farm = pd.read_csv(url)\n",
        "\n",
        "# ... and scale it as in the course\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(farm)\n",
        "farm_scaled = scaler.transform(farm)\n",
        "farm_scaled = pd.DataFrame(farm_scaled, columns=farm.columns)\n",
        "\n",
        "farm.head()"
      ],
      "metadata": {
        "id": "IT2QypVU0S03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca_farm = PCA(n_components=2)\n",
        "pca_farm.fit(farm_scaled)\n",
        "farm_pca = pca_farm.transform(farm_scaled)"
      ],
      "metadata": {
        "id": "4PS5NQtcVEgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explained_variance = pd.DataFrame()\n",
        "# Your code here"
      ],
      "metadata": {
        "id": "CueJXyP16I2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance = pd.DataFrame()\n",
        "# Your code here"
      ],
      "metadata": {
        "id": "W7tiJWRK6hUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "mask = # Your code here\n",
        "filtered = feature_importance[mask]\n",
        "sns.barplot(# Your code here)"
      ],
      "metadata": {
        "id": "YRC21Tnx682n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='purple'>**Optional (Advanced)**</font>  \n",
        "\n",
        "<font color='purple'>To determine the optimal number of components for a PCA, we can use what is called a \"scree plot\". A scree plot looks like follows:</font>\n",
        "\n",
        "<img src=\"https://www.statology.org/wp-content/uploads/2021/09/scree_plot_python.png\" alt=\"scree\" width=\"400\"/>\n",
        "\n",
        "<font color='purple'>On the y axis is the explained variance of every <i>additional</i> component. On the x axis is the number of components. The optimal number of components for a PCA is where the line of the scree plot \"bends\", i.e. where additional components do not contribute much to the overall explained variance.</font>\n",
        "\n",
        "<font color='purple'>Create such a scree plot to determine the optimal number of components for the farm data. For that, do the following:</font>\n",
        "\n",
        "<ol class=\"outside\">   \n",
        "<font color='purple'><li>Create two empty lists <i>comp</i> and <i>var</i>.</li>\n",
        "<li>Loop over range(1, 21) for components 1 to 20.</li>\n",
        "<li>Inititate and fit the PCA like above in each iteration of the loop.</li>\n",
        "<li>Store the number of components and explained variance for the last component in lists <i>comp</i> and <i>var</i> in each iteration. <b>Note</b>: You can use the <i>append</i> function. To obtain the explained variance of the additional component use <i>pca.explained_variance_ratio_[-1]</i>.</li>\n",
        "<li>Call the matplotlib function <i>plt.plot(comp, var, 'o-')</i> like so. Don't forget to <i>import matplotlib.pyplot as plt</i> first.</li></font>\n",
        "</ol>"
      ],
      "metadata": {
        "id": "wvEyrU2C4Y8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2\n",
        "In this exercise, we want to systematically investigate which number of clusters best fits the farm data. For this, use the data projected onto the first two principal components `farm_pca` from Exercise 1.\n",
        "\n",
        "* Use the K-Means algorithm and perform clustering for all numbers of clusters between `n_clusters=2` and `n_clusters=20`. Calculate the silhouette coefficient for each clustering and save the silhouette coefficient and the number of clusters in separate lists. **Note:** use a `for` loop. You can proceed very similarly to how we systematically evaluated classification performance in Course 4.\n",
        "* Plot the silhouette coefficient values against the number of clusters in a line chart. Which silhouette coefficient is the highest? Based on this, what number of clusters is best to divide the existing dataset into clusters?"
      ],
      "metadata": {
        "id": "khlYXX3tUtGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "# Your code here"
      ],
      "metadata": {
        "id": "kfrFlcq47hWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "sil_score_3 = silhouette_score(farm_pca, cluster)"
      ],
      "metadata": {
        "id": "pM9JsPED9zg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# lists for saving the results\n",
        "silhouette_coefficients = []\n",
        "number_clusters = []\n",
        "\n",
        "# Your code here"
      ],
      "metadata": {
        "id": "ProX-zCq8y_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.lineplot(# Your code here)"
      ],
      "metadata": {
        "id": "plzgSu9Z-iLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='purple'>**Optional (Advanced)**</font>  \n",
        "<font color='purple'>Remember that the k-means algorithm needs to be randomly initialized. That is, k-means needs to start from random cluster means and assign points to clusters first to then reassign points and refine clusters.\n",
        "\n",
        "Repeat the above calculations for different random seeds of k-means. For that, simply add a parameter `random_state` to `KMeans` and put an integer of your choice (e.g., `random_state=15`). Repeat the calculations for three seeds of your choice. How much do the results for the silhouette coefficient differ? Do you always get the same optimal number of clusters?</font>"
      ],
      "metadata": {
        "id": "1fb5Lti7-ST9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3\n",
        "\n",
        "In the [lecture](https://janalasser.at/lectures/MD_KI/VO3_4_algorithms_unsupervised_learning/#/4/4/5), we discussed that the K-Means algorithm does not work well when clusters are not \"round\" (i.e., do not have a convex shape). In such cases, density-based clustering can be an alternative.\n",
        "\n",
        "Below, a synthetic example dataset is generated to illustrate this problem:"
      ],
      "metadata": {
        "id": "-3Z9l77PE7UP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generating sample data with two \"half moons\" interfacing\n",
        "from sklearn import datasets\n",
        "moons = datasets.make_moons(n_samples=200, noise=0.05, random_state=42)[0]\n",
        "moons = pd.DataFrame(moons, columns=[\"x\", \"y\"])\n",
        "sns.scatterplot(moons, x=\"x\", y=\"y\");"
      ],
      "metadata": {
        "id": "617z9hWTBwXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We attempt to find clusters using the K-Means algorithm. Obviously, there are two groups of observations in the dataset, so we choose `n_clusters=2`."
      ],
      "metadata": {
        "id": "EwYHtIahEb1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clustering with K-Means algorthim and n_clusters=2\n",
        "kmeans_moons_2 = KMeans(n_clusters=2, random_state=42)\n",
        "kmeans_moons_2.fit(moons)\n",
        "cluster_moons_2 = kmeans_moons_2.predict(moons)\n",
        "\n",
        "# Creating a DataFrame for visualization\n",
        "plot_data_moons= moons.copy()\n",
        "plot_data_moons[\"cluster_2\"] = cluster_moons_2\n",
        "\n",
        "# Visualizing the clustering with a scatter plot\n",
        "sns.scatterplot(plot_data_moons, x=\"x\", y=\"y\", hue=\"cluster_2\");"
      ],
      "metadata": {
        "id": "vO7xIhZiCdg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "silhouette_score(moons, cluster_moons_2)"
      ],
      "metadata": {
        "id": "OKNohig6E4gU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although the silhouette coefficient is not bad, the clustering clearly does not match the structure of the data.\n",
        "\n",
        "To improve the situation, we apply density-based clustering. Scikit-learn provides the [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) algorithm for this. To use DBSCAN, we do **not** need to set any hyperparameters! In particular, DBSCAN automatically determines the optimal number of clusters.\n",
        "\n",
        "* Perform clustering on the `moons` data using `DBSCAN`, predict the cluster membership of each data point, and visualize the data with a scatter plot as in the course. How many clusters does DBSCAN find? **Note:** `DBSCAN` does not have separate `fit()` and `predict()` functions, but a combined `fit_predict()` function that takes the data as input and returns the clustering.\n",
        "* Scale the data using `StandardScaler()` as in the lecture and perform clustering again with `DBSCAN`. How many clusters are found now? Does the clustering meet your expectations? What is the silhouette coefficient of the resulting clustering?"
      ],
      "metadata": {
        "id": "EnI8r0NRE_dV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "dbscan = DBSCAN()\n",
        "# Your code here"
      ],
      "metadata": {
        "id": "WEOFbtgaC-YH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# Your code here"
      ],
      "metadata": {
        "id": "edPYfyWTDT1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "# Your code here"
      ],
      "metadata": {
        "id": "jr28b_zpId0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='purple'>**Optional (Advanced)**</font>  \n",
        "<font color='purple'>The silhouette coefficient is a statistical indicator to measure how \"good\" a clustering is. What disadvantages could there be when measuring the quality of a clustering purely with a statistical indicator? Can you think of other ways how to evalute how \"good\" or \"true\" the classes are that you found through clustering?</font>"
      ],
      "metadata": {
        "id": "qiDipeHGBe9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Source and License\n",
        "\n",
        "This notebook was created by Jana Lasser for Course \"B1 - Technical Aspects\" of the Microcredential \"AI and Society\" at the University of Graz.\n",
        "\n",
        "The notebook may be used, modified, and redistributed under the terms of the [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0) license.\n",
        "\n",
        "This notebook was translated from German using GPT-5 and cross-checked by Alina Herderich."
      ],
      "metadata": {
        "id": "f2GIomtM7kQv"
      }
    }
  ]
}