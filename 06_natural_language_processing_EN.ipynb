{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalue23/Exercises-Uni/blob/main/06_natural_language_processing_EN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing"
      ],
      "metadata": {
        "id": "d2Qs0BwXHbUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Objectives\n",
        "\n",
        "* Being able to transform text so that we can apply machine learning methods to it.\n",
        "* Being able to classify emotions in text.\n",
        "* Being able to evaluate the performance of a classifier that classifies more than two classes."
      ],
      "metadata": {
        "id": "gTE5eNugHd6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text as Data"
      ],
      "metadata": {
        "id": "VENRtAug7f1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using a dataset with English sentences, each assigned one of six possible emotions (sadness, joy, love, anger, fear, and surprise). The dataset is a subset of the [Emotions](https://www.kaggle.com/datasets/bhavikjikadara/emotions-dataset?resource=download) dataset, in which each emotion is equally represented (with 1,000 examples). We want to use this dataset to train a classifier that can recognize emotions in text.\n",
        "\n",
        "*The dataset may be used, modified, and published under a [CC BY 4.0 license](https://creativecommons.org/licenses/by/4.0/).*"
      ],
      "metadata": {
        "id": "28HSU7WoG1y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "url = \"https://drive.google.com/uc?id=1vfgHvGBMOAyozxlbwTGElPcyofeqjP9w\"\n",
        "emotions = pd.read_csv(url)\n",
        "emotions.head(10)"
      ],
      "metadata": {
        "id": "bDjxJTFpF5fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0: sadness\n",
        "# 1: joy\n",
        "# 2: love\n",
        "# 3: anger\n",
        "# 4: fear\n",
        "# 5: surprise"
      ],
      "metadata": {
        "id": "ugqtFx0XJ_qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotions[\"text_label\"].value_counts()"
      ],
      "metadata": {
        "id": "ZwPftPakGqEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we assign our observations (sentences) and target values (emotions)\n",
        "# to variables:\n",
        "x = emotions[\"text\"]\n",
        "y = emotions[\"label\"]"
      ],
      "metadata": {
        "id": "g0iwZWeAVvC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As discussed in the [lecture](https://janalasser.at/lectures/MD_KI/VO4_1_text_as_data/), we first need to transform the texts into numbers — that is, create an [embedding](https://janalasser.at/lectures/MD_KI/VO4_1_text_as_data/#/1/0/0) of the texts. One way to do this is by applying [one-hot encoding](https://janalasser.at/lectures/MD_KI/VO4_1_text_as_data/#/1) and treating each word as its own \"category.\"\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?id=1Bi6EPtynMNCraEnN780taAJkCQE8bjAF\" width=\"600\"/>\n",
        "</div>"
      ],
      "metadata": {
        "id": "A0-8agB_H3ov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we can use scikit-learn's CountVectorizer to achieve this\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "sentences = [\n",
        "    \"I love apples\",\n",
        "    \"I received a cat\",\n",
        "    \"tomorrow is Tuesday\"]\n",
        "\n",
        "count_vect = CountVectorizer()\n",
        "count_vect.fit(sentences)"
      ],
      "metadata": {
        "id": "Jro2pBzQQ2Z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for the example with only three sentences, the number of words (features)\n",
        "# is still manageable. In addition, the CountVectorizer ignores words\n",
        "# that have only one letter.\n",
        "count_vect.get_feature_names_out()"
      ],
      "metadata": {
        "id": "xB3ffktiROT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many dimensions (features) would the DataFrame have that we generate from the `emotions` dataset in this way? To find out, we can count the number of different words contained in the sentences of the dataset that have a length greater than 1:"
      ],
      "metadata": {
        "id": "jB7CusBUQ0-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list for saving the words\n",
        "all_words = []\n",
        "\n",
        "# iterate over all sentences in the data\n",
        "for sentence in emotions[\"text\"]:\n",
        "  # split the sentence into individual words based on spaces\n",
        "  words_in_sentence = sentence.split(\" \")\n",
        "  # iterate over all words of the current sentence\n",
        "  for word in words_in_sentence:\n",
        "    # if the word is not yet in the list: add\n",
        "    if word not in all_words and len(word) > 1:\n",
        "      all_words.append(word)\n",
        "\n",
        "# print the length of the resulting list\n",
        "print(f\"The dataset contains {len(all_words)} different words with a length greater than 1.\")"
      ],
      "metadata": {
        "id": "dUwoyDhRGWwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cross-check: the CountVectorizer also returns 8951 features\n",
        "count_vect = CountVectorizer()\n",
        "count_vect.fit(emotions['text'])\n",
        "print(f\"Number of features in CountVectorizer: {len(count_vect.get_feature_names_out())}\")"
      ],
      "metadata": {
        "id": "l_ip_bIaP8zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can refine the approach by ignoring words that occur rarely.\n",
        "# Here, we ignore all words that occur less than 3 times:\n",
        "count_vect = CountVectorizer(min_df=3)\n",
        "count_vect.fit(emotions['text'])\n",
        "print(f\"Number of features in CountVectorizer: {len(count_vect.get_feature_names_out())}\")"
      ],
      "metadata": {
        "id": "7026_4RtSx7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# That looks much more manageable! So we transform our dataset\n",
        "# with the CountVectorizer trained in this way:\n",
        "x_counts = count_vect.transform(x)"
      ],
      "metadata": {
        "id": "_AwFn5_lQLdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this dataset has 6,000 rows (observations) and 2,753 columns\n",
        "# (features, dimensions)\n",
        "x_counts.shape"
      ],
      "metadata": {
        "id": "q3NFcC8iQMt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what does a sentence transformed in this way look like?\n",
        "x_counts[0]"
      ],
      "metadata": {
        "id": "EpJmdXhqToBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# such large matrices are automatically stored as \"sparse matrices\"\n",
        "# to save memory. To display their content, we first need to\n",
        "# transform them into a regular matrix\n",
        "x_counts[0].todense()"
      ],
      "metadata": {
        "id": "9I4IXAp0TuBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many non-zero entries does the transformed first sentence have?\n",
        "x_counts[0].todense().sum()"
      ],
      "metadata": {
        "id": "c4WwdWrhUgqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what does the original sentence look like?\n",
        "# excluding words with length <2, there are 11 words in the sentence.\n",
        "# that means one word was dropped because it occurs less than 3 times\n",
        "# in the entire dataset.\n",
        "emotions[\"text\"].iloc[0]"
      ],
      "metadata": {
        "id": "6A_XB4hjUFla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Classifier for Emotion Recognition\n",
        "We now want to train a classifier with the dataset that can recognize emotions in an English sentence. For this, we follow the well-known train-test approach from [Session 4](https://colab.research.google.com/drive/1tkYspN1O9ehvcjKTyerLAISbpUG3XJB2?usp=sharing)."
      ],
      "metadata": {
        "id": "K3-m8YOfUwWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we split the dataset into a training set and a test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# we transform the observations in the training and test sets\n",
        "# with the previously trained CountVectorizer. Attention: it is important\n",
        "# that we trained the CountVectorizer on the entire dataset beforehand,\n",
        "# because there may be words that appear only in the training or\n",
        "# test set after splitting, which would otherwise not be transformed correctly\n",
        "x_train_counts = count_vect.transform(x_train)\n",
        "x_test_counts = count_vect.transform(x_test)"
      ],
      "metadata": {
        "id": "mV4ATxNDU4lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='blue'>**Exercise 1**</font>  \n",
        "<ul class=\"outside\">\n",
        "<li><font color='blue'>Train a <a href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">random forest classifier</a> using the training data. Follow the procedure from <a href=\"https://colab.research.google.com/drive/1tkYspN1O9ehvcjKTyerLAISbpUG3XJB2?usp=sharing\">Session 4</a> or the <a href=\"https://colab.research.google.com/drive/1hEHtlMQh794Lv8Th6J0VFksVBDFbbwm8?usp=sharing\">Session 4 homework</a>, Exercise 1 (6).</font></li>\n",
        "<li><font color='blue'>Predict the emotions for the test data.</font></li>\n",
        "<li><font color='blue'>Measure the accuracy of the predictions.</font></li>\n",
        "<li><font color='blue'>Predict the emotions for the four new sentences given in the code cell below. <b>Note</b>: don’t forget to transform the sentences first with the trained <tt>CountVectorizer</tt>!</font></li>\n",
        "<li><font color='blue'>Come up with four more sentences and predict their emotions. Do the predictions match your expectations?</font></li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "Sh0VJlcyWkge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "Xy6FwRaOXEFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiclass Classification\n",
        "\n",
        "So far, in supervised learning for classification, we have only dealt with binary classification (survive vs. not survive, breast cancer vs. no breast cancer). In the case of emotions, however, we have multiple classes (emotions). This is referred to as \"multiclass classification.\"\n",
        "\n",
        "It can also happen that we want to assign more than one class to an observation (not covered in this course). This is then referred to as \"multilabel classification.\""
      ],
      "metadata": {
        "id": "bwQu9PtGZQHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?id=1Sdftq2ZeBmJjzyHjrfJIMj4AnrDKIsrH\" width=\"800\"/>\n",
        "</div>"
      ],
      "metadata": {
        "id": "7ICVleYVcIFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Measuring the accuracy of a multiclass classifier is straightforward. We can simply count the cases where the classifier is incorrect.\n",
        "\n",
        "For other [performance metrics](https://janalasser.at/lectures/MD_KI/VO2_2_performance/#/2/0/2) like precision and recall, the procedure is not quite as simple — each class has its own precision and recall."
      ],
      "metadata": {
        "id": "b8reDkhbchYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier = RandomForestClassifier(verbose=True, random_state=0)\n",
        "classifier.fit(x_train_counts, y_train)\n",
        "y_pred = classifier.predict(x_test_counts)"
      ],
      "metadata": {
        "id": "E64bZN_4eN30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For which of the six emotions does the prediction work best?\n",
        "# 0: sadness\n",
        "# 1: joy\n",
        "# 2: love\n",
        "# 3: anger\n",
        "# 4: fear\n",
        "# 5: surprise\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "65IXuCg9eiCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='blue'>**Exercise 2**</font>  \n",
        "<ul class=\"outside\">\n",
        "<li><font color='blue'>Each person chooses an emotion and writes 10 sentences that are meant to express that emotion in this <a href=\"https://docs.google.com/document/d/1-U_YxRfU0q10_rLh_gFrwUqaRhBqv9znYUwQfqGdoo4/edit?usp=sharing\">Google Doc</a>.</font></li>\n",
        "<li><font color='blue'>Copy all sentences into the list <tt>x_test_new</tt> in the code cell below. Pay attention to the order: first all sentences for \"sadness\", then all for \"joy\", etc.</font></li>\n",
        "<li><font color='blue'>Predict the emotion for all the new sentences. <b>Note</b>: don’t forget to transform the sentences first with the trained <tt>CountVectorizer</tt>!</font></li>\n",
        "<li><font color='blue'>Measure the classifier's performance on the new sentences by creating a <tt>classification_report</tt>. How well does the classifier perform on this completely new dataset, i.e., \"out of sample\"?</font></li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "xzPk-yT4g1CY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_neu = [\n",
        "    # Copy the sentences here\n",
        "]\n",
        "\n",
        "y_test_neu = [\n",
        "    # Copy the targets here\n",
        "]"
      ],
      "metadata": {
        "id": "r3Sjtj1chVoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "GMCNmzWISDyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Materials\n",
        "* **Embeddings with Pretrained Models**: [NLTK tutorial](https://www.nltk.org/howto/gensim.html)\n",
        "* **Micro- and Macro F1 Score Explained**: [Blog post](https://iamirmasoud.com/2022/06/19/understanding-micro-macro-and-weighted-averages-for-scikit-learn-metrics-in-multi-class-classification-with-example/)\n",
        "* **How LLMs Work**: [Video](https://www.youtube.com/watch?v=wjZofJX0v4M) by 3Blue1Brown (also includes a brief section on embeddings)\n",
        "* **Emotion Recognition and the AI Act**: [Comment](https://www.technologyslegaledge.com/2025/04/eu-ai-act-spotlight-on-emotional-recognition-systems-in-the-workplace/) on emotion recognition systems in the AI Act"
      ],
      "metadata": {
        "id": "1s3FxpXzmSA6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Source and License\n",
        "\n",
        "This notebook was created by Jana Lasser for Course \"B1 - Technical Aspects\" of the Microcredential \"AI and Society\" at the University of Graz.\n",
        "\n",
        "The notebook may be used, modified, and redistributed under the terms of the [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0) license.\n",
        "\n",
        "This notebook was translated from German using GPT-5 and cross-checked by Alina Herderich."
      ],
      "metadata": {
        "id": "HF-P2SdOEhCg"
      }
    }
  ]
}